# üìò Parte 4 ‚Äì Gerenciamento de Mem√≥ria no LangChain

## 1. Vis√£o geral da parte

O m√≥dulo **Parte 4 ‚Äì Gerenciamento de Mem√≥ria** tem como objetivo introduzir e aprofundar o conceito de **mem√≥ria conversacional** em aplica√ß√µes constru√≠das com LangChain. At√© aqui, o estudo focou em prompts, chains, modelos e agentes de forma majoritariamente *stateless* ‚Äî ou seja, cada chamada ao modelo era independente.

Nesta parte, passamos a entender **como e por que manter hist√≥rico de conversas**, explorando como o contexto acumulado influencia diretamente o comportamento, a coer√™ncia e a utilidade das respostas de um LLM.

### Problemas que esta parte resolve

* Como permitir que o modelo **lembre informa√ß√µes fornecidas anteriormente** pelo usu√°rio
* Como estruturar aplica√ß√µes conversacionais reais, em vez de chamadas isoladas ao LLM
* Como **controlar o crescimento do hist√≥rico**, evitando:

  * Custos excessivos de tokens
  * Vazamento de informa√ß√µes antigas ou irrelevantes
  * Respostas enviesadas por contexto obsoleto

### Conex√£o com as pr√≥ximas partes

O gerenciamento de mem√≥ria √© um **pr√©-requisito fundamental** para:

* Agentes com m√∫ltiplos passos e decis√µes
* Ferramentas que dependem de contexto cont√≠nuo
* Aplica√ß√µes multiusu√°rio (sessions)
* Persist√™ncia de mem√≥ria (bancos de dados, Redis, etc.)

Nas pr√≥ximas partes, esses conceitos evoluem naturalmente para **agentes**, **tools**, **mem√≥ria persistente** e **arquiteturas de produ√ß√£o**.

---

## 2. Explica√ß√£o detalhada dos arquivos

### üìÑ 1-armazenamento-de-historico.py

#### O que este arquivo faz

Este exemplo demonstra a forma **mais direta e intuitiva** de trabalhar com mem√≥ria no LangChain:

* Todo o hist√≥rico da conversa √© armazenado
* O modelo tem acesso completo a todas as mensagens anteriores
* O LLM consegue ‚Äúlembrar‚Äù informa√ß√µes ditas pelo usu√°rio

√â o equivalente a uma conversa cont√≠nua, sem nenhum tipo de poda ou limita√ß√£o.

#### Conceito demonstrado

* `InMemoryChatMessageHistory`
* `RunnableWithMessageHistory`
* Uso expl√≠cito de `MessagesPlaceholder` em prompts

Esse √© o **padr√£o base de mem√≥ria conversacional** no LangChain.

---

#### Estrutura do c√≥digo por blocos

##### 1. Carregamento de depend√™ncias e vari√°veis de ambiente

```python
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables import RunnableWithMessageHistory

load_dotenv()
```

* `load_dotenv()` carrega credenciais da API
* O modelo usado √© um **chat model**, n√£o um completion model

---

##### 2. Defini√ß√£o do prompt com espa√ßo para hist√≥rico

```python
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}"),
])
```

Pontos-chave:

* `MessagesPlaceholder` indica ao LangChain **onde inserir o hist√≥rico**
* O hist√≥rico ser√° uma lista de mensagens (human/assistant/system)
* A ordem importa: hist√≥rico vem **antes** do input atual

---

##### 3. Inicializa√ß√£o do modelo

```python
chat_model = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.9
)
```

* Modelo de chat
* `temperature` alta favorece respostas mais criativas

---

##### 4. Cria√ß√£o da chain base

```python
chain = prompt | chat_model
```

Aqui ainda **n√£o existe mem√≥ria**. √â apenas uma chain comum.

---

##### 5. Armazenamento de sess√µes

```python
session_store: dict[str, InMemoryChatMessageHistory] = {}


def get_session_history(session_id: str) -> InMemoryChatMessageHistory:
    if session_id not in session_store:
        session_store[session_id] = InMemoryChatMessageHistory()
    return session_store[session_id]
```

* Cada `session_id` possui seu pr√≥prio hist√≥rico
* Isso permite m√∫ltiplos usu√°rios ou conversas paralelas

---

##### 6. Envolvendo a chain com mem√≥ria

```python
conversational_chain = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history",
)
```

Esse √© o **ponto central do exemplo**:

* `RunnableWithMessageHistory` injeta automaticamente:

  * Mensagens do usu√°rio
  * Respostas do modelo
* O hist√≥rico cresce a cada intera√ß√£o

---

##### 7. Execu√ß√£o com sess√£o fixa

```python
config = {"configurable": {"session_id": "demo-session"}}
```

Todas as chamadas usam o mesmo `session_id`, garantindo continuidade.

---

#### Resultado observado

O modelo:

* Aprende o nome do usu√°rio
* Consegue repeti-lo
* Usa a informa√ß√£o em respostas futuras

Isso confirma que o **hist√≥rico completo est√° sendo preservado**.

---

### üìÑ 2-historico-baseado-em-sliding-window.py

#### O que este arquivo faz

Este exemplo introduz uma estrat√©gia mais avan√ßada:

* O hist√≥rico completo **existe**, mas
* Apenas uma **janela limitada** √© passada ao modelo

O objetivo √© demonstrar **controle fino sobre contexto**.

---

#### Conceito demonstrado

* Sliding window de mem√≥ria
* `trim_messages`
* Separa√ß√£o entre:

  * Hist√≥rico bruto armazenado
  * Hist√≥rico efetivamente enviado ao LLM

---

#### Estrutura do c√≥digo por blocos

##### 1. Prompt com hist√≥rico

```python
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant that answers with a short joke when possible."),
    MessagesPlaceholder("history"),
    ("human", "{input}"),
])
```

Semelhante ao exemplo anterior, mas com um comportamento de sistema diferente.

---

##### 2. Fun√ß√£o de prepara√ß√£o de inputs

```python
def prepare_inputs(payload: dict) -> dict:
    raw_history = payload.get("raw_history", [])
    trimmed = trim_messages(
        raw_history,
        token_counter=len,
        max_tokens=2,
        strategy="last",
        start_on="human",
        include_system=True,
        allow_partial=False,
    )
    return {
        "input": payload.get("input", ""),
        "history": trimmed
    }
```

Aqui est√° o **cora√ß√£o da sliding window**:

* `raw_history`: hist√≥rico completo armazenado
* `trim_messages`: reduz o hist√≥rico
* Apenas as √∫ltimas mensagens relevantes s√£o mantidas

Importante:

* O modelo **n√£o v√™ tudo**
* Ele s√≥ responde com base na janela ativa

---

##### 3. Uso de RunnableLambda

```python
prepare = RunnableLambda(prepare_inputs)
chain = prepare | prompt | llm
```

* Permite pr√©-processamento antes do prompt
* Padr√£o extremamente comum em pipelines reais

---

##### 4. Mem√≥ria com chave diferente

```python
conversational_chain = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="raw_history"
)
```

Diferente do primeiro exemplo:

* O hist√≥rico armazenado chama-se `raw_history`
* O hist√≥rico enviado ao prompt chama-se `history`

Essa separa√ß√£o √© **intencional e poderosa**.

---

#### Resultado observado

Quando o usu√°rio pergunta seu nome:

* O modelo **n√£o sabe responder**
* A informa√ß√£o foi descartada pela sliding window

Isso demonstra claramente:

> Mem√≥ria armazenada ‚â† mem√≥ria usada pelo modelo

---

## 3. Compara√ß√£o entre os arquivos

| Aspecto                   | Hist√≥rico Completo   | Sliding Window    |
| ------------------------- | -------------------- | ----------------- |
| Simplicidade              | Alta                 | M√©dia             |
| Controle de contexto      | Nenhum               | Alto              |
| Custo de tokens           | Crescente            | Controlado        |
| Risco de info irrelevante | Alto                 | Baixo             |
| Casos de uso              | Demos, chats simples | Produ√ß√£o, agentes |

### Quando usar cada abordagem

* **Hist√≥rico completo**:

  * Tutoriais
  * Prototipa√ß√£o
  * Conversas curtas

* **Sliding window**:

  * Chats longos
  * Sistemas multiusu√°rio
  * Agentes aut√¥nomos

---

## 4. Aspectos dependentes vs independentes de modelo

### Dependentes de modelo

* Uso de `ChatPromptTemplate`
* Mensagens estruturadas (human/system/assistant)
* Necessidade de modelos do tipo *chat*

### Independentes de modelo (abstra√ß√µes LangChain)

* `RunnableWithMessageHistory`
* Estrat√©gias de mem√≥ria
* Pipelines com `RunnableLambda`

### Boas pr√°ticas de desacoplamento

* Isolar prompts
* Isolar mem√≥ria
* Trocar modelos sem reescrever l√≥gica

---

## 5. Boas pr√°ticas e dicas

### Erros comuns

* N√£o limitar hist√≥rico
* Confiar que o modelo ‚Äúvai lembrar‚Äù sem mem√≥ria
* Misturar hist√≥rico bruto com hist√≥rico de prompt

### Padr√µes recomendados

* Sempre separar:

  * Armazenamento
  * Contexto ativo
* Usar sliding window por padr√£o
* Aumentar janela apenas quando necess√°rio

### Evolu√ß√£o para aplica√ß√µes reais

* Substituir `InMemoryChatMessageHistory`
* Persistir em banco ou cache
* Combinar com ferramentas e agentes

---

## 6. Resumo final

### Principais aprendizados

* Mem√≥ria n√£o √© autom√°tica em LLMs
* LangChain fornece abstra√ß√µes claras para isso
* Hist√≥rico completo √© simples, mas perigoso
* Sliding window equilibra contexto, custo e relev√¢ncia

### Checklist mental do aluno

* [x] Sei o que √© mem√≥ria conversacional
* [x] Sei usar `RunnableWithMessageHistory`
* [x] Sei controlar hist√≥rico com sliding window
* [x] Sei quando descartar contexto antigo
* [x] Estou pronto para agentes com mem√≥ria
