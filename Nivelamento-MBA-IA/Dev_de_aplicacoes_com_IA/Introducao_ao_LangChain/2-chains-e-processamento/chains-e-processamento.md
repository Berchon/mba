# üìò Parte 2 ‚Äì Chains e Processamento no LangChain

## üéØ Vis√£o Geral da Parte

A **Parte 2 ‚Äì Chains e Processamento** marca uma transi√ß√£o fundamental no estudo de LangChain: aqui deixamos de utilizar modelos de linguagem como chamadas isoladas e passamos a **compor fluxos de processamento estruturados**, nos quais m√∫ltiplas etapas trabalham em conjunto.

Nesta parte, o foco est√° em:

* Pensar em **LLMs como transformadores de dados** dentro de um pipeline
* Construir **chains reutiliz√°veis e declarativas**
* Combinar **prompts, fun√ß√µes Python, modelos e parsers** em fluxos coerentes
* Lidar com **textos longos**, limita√ß√µes de contexto e estrat√©gias como *map-reduce*

### Problemas que esta parte resolve

* Uso repetitivo e n√£o estruturado de chamadas a LLMs
* Dificuldade de reutilizar l√≥gica de prompts e processamento
* Falta de clareza sobre como encadear m√∫ltiplas transforma√ß√µes
* Limita√ß√µes de contexto ao processar textos longos

### Conex√£o com as pr√≥ximas partes

Os conceitos apresentados aqui s√£o **pr√©-requisitos diretos** para:

* **RAG (Retrieval-Augmented Generation)**: pipelines com recupera√ß√£o + gera√ß√£o
* **Agents**: chains como etapas internas de tomada de decis√£o
* **Automa√ß√£o com IA**: fluxos declarativos e reutiliz√°veis

---

## üß© Arquivo 1 ‚Äì `1-iniciando-com-chains.py`

### O que este arquivo faz

Este arquivo apresenta a forma mais simples de criar uma **chain declarativa** usando o **LCEL (LangChain Expression Language)**, conectando um prompt diretamente a um modelo.

### Conceito demonstrado

* `PromptTemplate`
* Chat Model (`ChatGoogleGenerativeAI`)
* Operador `|` para composi√ß√£o de chains

### C√≥digo explicado

```python
question_template = PromptTemplate(
    input_variables=["name"],
    template="Hi, I'm {name}! Tell me a joke about my name!"
)
```

Define um prompt parametrizado. Aqui, o prompt √© uma **fun√ß√£o declarativa** que recebe dados de entrada.

```python
model = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.5)
```

Inicializa um **Chat Model**, respons√°vel pela gera√ß√£o de texto.

```python
chain = question_template | model
```

Cria uma **chain**: a sa√≠da do prompt alimenta diretamente o modelo.

```python
result = chain.invoke({"name": "Aldebaran"})
```

Executa a chain com os dados de entrada.

### Conceitos importantes

* LCEL
* Encadeamento declarativo
* Separa√ß√£o entre prompt e modelo

---

## üß© Arquivo 2 ‚Äì `2-chains-com-decorators.py`

### O que este arquivo faz

Demonstra como **fun√ß√µes Python comuns** podem se tornar etapas de uma chain usando o decorator `@chain`.

### Conceito demonstrado

* `@chain`
* Fun√ß√µes como etapas de processamento
* Integra√ß√£o de l√≥gica determin√≠stica com LLMs

### C√≥digo explicado

```python
@chain
def square(input_dict: dict) -> dict:
    x = input_dict["x"]
    return {"square_result": x * x}
```

Transforma uma fun√ß√£o Python em um **Runnable**, compat√≠vel com o LCEL.

```python
chain2 = square | question_template2 | model
```

Aqui temos um pipeline h√≠brido:

1. C√°lculo determin√≠stico
2. Prompt
3. Gera√ß√£o por LLM

### Conceitos importantes

* Runnables
* Pipelines h√≠bridos (Python + LLM)

---

## üß© Arquivo 3 ‚Äì `3-runnable_lambda.py`

### O que este arquivo faz

Mostra como criar um Runnable a partir de uma fun√ß√£o usando `RunnableLambda`.

### Conceito demonstrado

* `RunnableLambda`
* Adapta√ß√£o de fun√ß√µes existentes

### C√≥digo explicado

```python
parse_runnable = RunnableLambda(parse_number)
```

Permite encapsular uma fun√ß√£o simples como parte de um pipeline LCEL.

### Quando usar

* Fun√ß√µes definidas em outros m√≥dulos
* C√≥digo legado
* Quando n√£o √© poss√≠vel usar decorators

---

## üß© Arquivo 4 ‚Äì `4-pipeline-de-processamento.py`

### O que este arquivo faz

Cria um **pipeline multi-etapas**, no qual a sa√≠da de uma chain alimenta outra.

### Conceito demonstrado

* Pipelines com dicion√°rios
* `StrOutputParser`
* Composi√ß√£o de chains

### Destaque conceitual

```python
pipeline = {"text_to_summarize": translate} | template_summary | model | StrOutputParser()
```

Aqui, o LangChain atua como um **orquestrador de fluxo de dados**.

---

## üß© Arquivo 5 ‚Äì `5-sumarizacao.py`

### O que este arquivo faz

Este arquivo demonstra o uso de uma **cadeia de sumariza√ß√£o pronta** do LangChain utilizando a estrat√©gia **`stuff`**. Ele representa a forma mais direta e simples de resumir um texto usando LangChain.

A ideia central √©:

1. Dividir (opcionalmente) um texto longo em partes menores
2. **Juntar todo o conte√∫do em um √∫nico prompt**
3. Enviar esse prompt completo para o modelo gerar o resumo

Mesmo utilizando um `TextSplitter`, neste exemplo **todas as partes s√£o reunidas novamente** antes da chamada ao modelo. O uso do splitter aqui √© apenas did√°tico.

### Estrat√©gia `stuff`: como funciona

Na estrat√©gia `stuff`, o LangChain:

* Pega todos os documentos (chunks)
* "Enfia" (stuff) todo o conte√∫do em um √∫nico prompt
* Executa **uma √∫nica chamada ao LLM**

Fluxo conceitual:

```
Documentos ‚Üí [concatena√ß√£o] ‚Üí Prompt √∫nico ‚Üí LLM ‚Üí Resumo
```

### Quando usar `stuff`

Esta estrat√©gia √© recomendada quando:

* O texto **cabe confortavelmente no contexto do modelo**
* Voc√™ quer **simplicidade m√°xima**
* O custo de m√∫ltiplas chamadas ao modelo n√£o se justifica
* O resumo n√£o exige processamento incremental

### Limita√ß√µes importantes

* N√£o escala bem para textos grandes
* Pode estourar o limite de tokens do modelo
* Menor controle sobre o processo intermedi√°rio

### Conceitos importantes envolvidos

* Cadeias prontas (`load_summarize_chain`)
* Limites de contexto de LLMs
* Trade-off entre simplicidade e escala

---

## üß© Arquivo 6 ‚Äì `6-sumarizacao-com-map-reduce.py`

### O que este arquivo faz

Este arquivo demonstra a mesma tarefa de sumariza√ß√£o, por√©m utilizando a estrat√©gia **`map_reduce`**, que √© mais robusta e escal√°vel para textos longos.

Aqui, o LangChain aplica explicitamente o padr√£o **Map-Reduce**, muito comum em processamento distribu√≠do.

### Estrat√©gia `map_reduce`: como funciona

A estrat√©gia √© dividida em duas fases claras:

#### 1Ô∏è‚É£ Fase Map

* Cada chunk de texto √© enviado **individualmente** ao LLM
* O modelo gera um **resumo parcial** para cada parte

#### 2Ô∏è‚É£ Fase Reduce

* Todos os resumos parciais s√£o combinados
* Um novo prompt √© enviado ao modelo para gerar o **resumo final**

Fluxo conceitual:

```
Documentos ‚Üí Map (resumos parciais) ‚Üí Reduce (resumo final)
```

### Quando usar `map_reduce`

Esta estrat√©gia √© recomendada quando:

* O texto √© **grande ou potencialmente ilimitado**
* H√° risco real de ultrapassar o contexto do modelo
* Voc√™ precisa de **robustez e escalabilidade**
* O custo de m√∫ltiplas chamadas ao modelo √© aceit√°vel

### Vantagens em rela√ß√£o ao `stuff`

* Escala para textos muito grandes
* Evita estouro de contexto
* Processo mais previs√≠vel

### Desvantagens

* Mais chamadas ao modelo (maior custo)
* Mais lat√™ncia
* Menos simples conceitualmente para iniciantes

### Conceitos importantes envolvidos

* Map-Reduce
* Processamento incremental
* Cadeias compostas

---

## üß© Arquivo 7 ‚Äì `7-pipeline-de-sumarizacao.py`

### O que este arquivo faz

Este arquivo reimplementa **manualmente** a estrat√©gia de map-reduce utilizando apenas **LCEL (LangChain Expression Language)**, sem usar cadeias prontas.

O objetivo aqui n√£o √© simplicidade, mas **controle total e compreens√£o profunda** do fluxo.

### Por que este exemplo √© importante

Ele mostra que:

* Cadeias prontas s√£o apenas **abstra√ß√µes de conveni√™ncia**
* Todo o comportamento pode ser reproduzido com LCEL
* LangChain √© um **framework de orquestra√ß√£o**, n√£o apenas prompts prontos

### Estrutura detalhada do pipeline

#### 1Ô∏è‚É£ Prepara√ß√£o dos dados (Split)

O texto √© dividido em chunks usando `TextSplitter`, assim como nos exemplos anteriores.

#### 2Ô∏è‚É£ Fase Map com LCEL

```python
map_chain = map_prompt | llm | StrOutputParser()
```

Cada chunk passa por:

* Um prompt de resumo
* O modelo
* Um parser de sa√≠da

O m√©todo `.map()` aplica essa chain a **cada chunk individualmente**.

#### 3Ô∏è‚É£ Prepara√ß√£o para o Reduce

```python
RunnableLambda(lambda summaries: [{"context": "
".join(summaries)}])
```

Aqui ocorre uma transforma√ß√£o puramente Python, preparando os dados para o pr√≥ximo est√°gio.

#### 4Ô∏è‚É£ Fase Reduce

Um novo prompt combina todos os resumos parciais em um resumo final.

### Quando usar este tipo de pipeline

Este padr√£o √© recomendado quando:

* Voc√™ precisa de **controle fino** sobre cada etapa
* Deseja inserir l√≥gica customizada entre map e reduce
* Quer instrumentar, logar ou validar etapas intermedi√°rias
* Est√° construindo sistemas complexos (RAG, agents, workflows)

### Compara√ß√£o com cadeias prontas

| Aspecto       | Cadeias prontas | LCEL manual |
| ------------- | --------------- | ----------- |
| Simplicidade  | Alta            | M√©dia/Baixa |
| Controle      | Baixo           | Alto        |
| Flexibilidade | Limitada        | M√°xima      |
| Uso did√°tico  | In√≠cio          | Avan√ßado    |

### Conceitos importantes envolvidos

* LCEL avan√ßado
* `map()`
* Pipelines declarativos complexos

### Stuff vs Map_Reduce

|              |                                |               |
| ------------ | ------------------------------ | ------------- |
| `stuff`      | Junta tudo em um √∫nico prompt  | Textos curtos |
| `map_reduce` | Resume partes e depois combina | Textos longos |

### Conceitos importantes

* `TextSplitter`
* Limita√ß√µes de contexto
* Estrat√©gias de redu√ß√£o

---

## üîç Compara√ß√£o entre os arquivos

| Abordagem      | N√≠vel         | Controle | Uso t√≠pico         |
| -------------- | ------------- | -------- | ------------------ |
| Chain simples  | B√°sico        | Baixo    | Introdu√ß√£o         |
| Decorators     | Intermedi√°rio | M√©dio    | Pipelines h√≠bridos |
| Chains prontas | Intermedi√°rio | Baixo    | Produtividade      |
| LCEL puro      | Avan√ßado      | Alto     | Sistemas complexos |

---

## ‚öôÔ∏è Depend√™ncia de modelo vs abstra√ß√£o

### Dependente de modelo

* Tipo de sa√≠da (`AIMessage`)
* Limita√ß√µes de contexto
* Par√¢metros como `temperature`

### Independente de modelo

* LCEL (`|`, `.map()`)
* Runnables
* Prompts
* Parsers

### Boas pr√°ticas

* Use `StrOutputParser` sempre que poss√≠vel
* Evite acessar `.content` diretamente
* Injete o modelo no n√≠vel mais externo poss√≠vel

---

## ‚úÖ Boas pr√°ticas e dicas

* Pense em chains como **fun√ß√µes puras**
* Separe l√≥gica determin√≠stica de gera√ß√£o
* Prefira pipelines declarativos
* Use `map_reduce` para escala

---

## üß† Resumo Final

### Principais aprendizados

* Chains s√£o pipelines de transforma√ß√£o de dados
* LLMs s√£o apenas uma etapa do fluxo
* LCEL permite composi√ß√£o clara e reutiliz√°vel

### Checklist mental

* [x] Sei criar chains simples
* [x] Sei integrar fun√ß√µes Python
* [x] Sei processar textos longos
* [x] Sei escolher entre chains prontas e LCEL puro

‚úîÔ∏è Com isso, voc√™ est√° preparado para avan√ßar para RAG, Agents e aplica√ß√µes reais.
