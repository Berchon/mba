# ğŸ“˜ Parte 5 â€“ Loaders e Bancos de Dados Vetoriais no LangChain

Este documento faz parte de um estudo introdutÃ³rio e progressivo sobre **LangChain**, com foco em **carregamento de dados, geraÃ§Ã£o de embeddings e recuperaÃ§Ã£o semÃ¢ntica usando bancos de dados vetoriais**.

O conteÃºdo foi elaborado para leitores com **conhecimento bÃ¡sico de Python**, mas **iniciantes em LangChain e em arquiteturas de RAG (Retrieval-Augmented Generation)**.

---

## 1. VisÃ£o Geral da Parte

### ğŸ¯ Objetivo desta parte

A Parte 5 tem como objetivo ensinar **como transformar dados textuais brutos em uma base vetorial consultÃ¡vel semanticamente**. Para isso, o mÃ³dulo demonstra um pipeline completo de:

1. **Carregamento de documentos** a partir de diferentes fontes (web e PDF)
2. **DivisÃ£o dos textos em chunks** adequados para LLMs
3. **GeraÃ§Ã£o de embeddings de linguagem natural**
4. **PersistÃªncia desses vetores em um banco vetorial (PGVector / PostgreSQL)**
5. **Consulta semÃ¢ntica por similaridade**

Esse fluxo representa a espinha dorsal de sistemas modernos de busca semÃ¢ntica, chatbots com memÃ³ria documental e aplicaÃ§Ãµes RAG.

---

### â“ Quais problemas esta parte resolve

Sem esse tipo de abordagem, aplicaÃ§Ãµes baseadas em LLMs enfrentam limitaÃ§Ãµes sÃ©rias:

* LLMs nÃ£o â€œlembramâ€ documentos grandes
* Contexto enviado no prompt Ã© limitado por tokens
* Busca por palavras-chave nÃ£o captura significado semÃ¢ntico

O uso de **embeddings + banco vetorial** resolve esses problemas ao permitir:

* IndexaÃ§Ã£o de grandes volumes de texto
* Busca baseada em significado, nÃ£o em palavras exatas
* RecuperaÃ§Ã£o eficiente de trechos relevantes

---

### ğŸ”— ConexÃ£o com as prÃ³ximas partes

Esta parte prepara o terreno para mÃ³dulos mais avanÃ§ados, como:

* **RAG completo** (Retriever + LLM)
* **Chains que combinam busca vetorial com geraÃ§Ã£o de respostas**
* **Agentes que consultam bases de conhecimento externas**
* **MemÃ³ria de longo prazo baseada em vetores**

Sem dominar loaders, splitters e bancos vetoriais, essas arquiteturas nÃ£o escalam corretamente.

---

## 2. ExplicaÃ§Ã£o Detalhada de Cada Arquivo

---

## ğŸ“„ Arquivo 1 â€“ `1-carregamento-usando-WebBaseLoader.py`

### ğŸ“Œ O que este arquivo faz

Este script demonstra como:

* Carregar conteÃºdo textual diretamente de uma pÃ¡gina web
* Transformar esse conteÃºdo em documentos do LangChain
* Dividir o texto em chunks menores usando um splitter recursivo

---

### ğŸ§  Conceito do LangChain demonstrado

* **Document Loaders**
* **WebBaseLoader**
* **RecursiveCharacterTextSplitter**

---

### ğŸ§© ExplicaÃ§Ã£o por blocos

```python
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
```

ImportaÃ§Ã£o dos componentes responsÃ¡veis por:

* Carregar dados da web
* Dividir textos longos em fragmentos menores

---

```python
loader = WebBaseLoader("https://www.langchain.com/")
docs = loader.load()
```

* O `WebBaseLoader` faz uma requisiÃ§Ã£o HTTP simples
* Extrai o conteÃºdo textual principal da pÃ¡gina
* Retorna uma lista de objetos `Document`

> Cada `Document` contÃ©m:
>
> * `page_content`: texto
> * `metadata`: URL, fonte, etc.

---

```python
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = splitter.split_documents(docs)
```

* O splitter quebra o texto em pedaÃ§os de atÃ© 500 caracteres
* MantÃ©m sobreposiÃ§Ã£o de 100 caracteres entre chunks
* O mÃ©todo recursivo tenta preservar limites semÃ¢nticos (parÃ¡grafos, frases)

---

```python
for chunk in chunks:
    print(chunk)
    print("-"*30)
```

* Apenas visualiza os chunks gerados
* Ãštil para entender como o texto foi fragmentado

---

### âš ï¸ ObservaÃ§Ã£o importante

O prÃ³prio cÃ³digo comenta corretamente:

> Para casos complexos de crawling e scraping, bibliotecas especializadas (Scrapy, BeautifulSoup, Firecrawl, etc.) sÃ£o mais adequadas.

O `WebBaseLoader` Ã© **simples e didÃ¡tico**, nÃ£o um crawler completo.

---

## ğŸ“„ Arquivo 2 â€“ `2-carregamento-de-pdf.py`

### ğŸ“Œ O que este arquivo faz

Este script demonstra como:

* Carregar um documento PDF local
* Converter pÃ¡ginas do PDF em documentos
* Dividir o conteÃºdo em chunks

---

### ğŸ§  Conceito do LangChain demonstrado

* **PyPDFLoader**
* **Loaders para arquivos locais**
* **Processamento de documentos estruturados**

---

### ğŸ§© ExplicaÃ§Ã£o por blocos

```python
from pathlib import Path
```

Uso do `Pathlib` para manipulaÃ§Ã£o segura de caminhos de arquivos.

---

```python
BASE_DIR = Path(__file__).resolve().parent
pdf_path = BASE_DIR / "gpt5.pdf"
```

* Garante que o script funcione independentemente do diretÃ³rio de execuÃ§Ã£o
* Boa prÃ¡tica para scripts reutilizÃ¡veis

---

```python
loader = PyPDFLoader(str(pdf_path))
docs = loader.load()
```

* Cada pÃ¡gina do PDF vira um objeto `Document`
* Metadados incluem nÃºmero da pÃ¡gina e origem

---

```python
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = splitter.split_documents(docs)
```

* Mesma estratÃ©gia usada para conteÃºdo web
* ReforÃ§a que **splitters sÃ£o independentes da fonte**

---

```python
print(len(chunks))
```

* Apenas valida o volume de fragmentos gerados

---

## ğŸ“„ Arquivo 3 â€“ `3-ingestion-pgvector.py`

### ğŸ“Œ O que este arquivo faz

Este Ã© o script mais importante da parte. Ele implementa a **ingestÃ£o completa em um banco vetorial**:

* Carrega um PDF
* Divide em chunks
* Gera embeddings
* Persiste os vetores no PostgreSQL usando PGVector

---

### ğŸ§  Conceitos demonstrados

* **Embeddings**
* **Vector Stores**
* **PGVector**
* **IntegraÃ§Ã£o com serviÃ§os externos**

---

### ğŸ§© ExplicaÃ§Ã£o por blocos

```python
from dotenv import load_dotenv
load_dotenv()
```

Carrega variÃ¡veis sensÃ­veis a partir de `.env` â€” prÃ¡tica essencial em projetos reais.

---

```python
for k in ["GOOGLE_API_KEY", "PGVECTOR_URL", "PGVECTOR_COLLECTION"]:
    if k not in os.environ:
        raise RuntimeError(...)
```

* ValidaÃ§Ã£o explÃ­cita de dependÃªncias de ambiente
* Falha rÃ¡pida (fail fast)

---

```python
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=150,
)
```

* Chunks maiores para embeddings
* Overlap maior preserva contexto semÃ¢ntico

---

```python
enriched = [Document(
    page_content=doc.page_content,
    metadata={k: v for k, v in doc.metadata.items() if v not in ("", None)},
) for doc in splits]
```

* Limpa metadados invÃ¡lidos
* Prepara documentos para persistÃªncia

---

```python
embeddings = GoogleGenerativeAIEmbeddings(
    model="text-embedding-004",
)
```

* Define explicitamente o modelo de embeddings
* AbstraÃ§Ã£o permite trocar o provedor futuramente

---

```python
store = PGVector(
    embeddings=embeddings,
    collection_name=...,
    connection=...,
    use_jsonb=True,
)
```

* Inicializa o banco vetorial
* `use_jsonb=True` melhora performance e flexibilidade

---

```python
store.add_documents(documents=enriched, ids=ids)
```

* Persiste vetores + metadados
* Conclui o pipeline de ingestÃ£o

---

## ğŸ“„ Arquivo 4 â€“ `4-search-vector.py`

### ğŸ“Œ O que este arquivo faz

Este script demonstra como:

* Consultar um banco vetorial existente
* Realizar busca por similaridade semÃ¢ntica
* Recuperar documentos relevantes

---

### ğŸ§  Conceitos demonstrados

* **Similarity Search**
* **Embeddings de consulta**
* **Scoring semÃ¢ntico**

---

### ğŸ§© ExplicaÃ§Ã£o por blocos

```python
query = "Tell me more about the gpt-5 thinking evaluation..."
```

Consulta em linguagem natural â€” nÃ£o precisa coincidir com o texto original.

---

```python
results = store.similarity_search_with_score(query, k=3)
```

* Retorna os 3 documentos mais similares
* Cada resultado inclui um score de distÃ¢ncia

---

```python
for doc, score in results:
    print(doc.page_content)
```

* Visualiza texto recuperado
* Metadados ajudam no rastreamento da origem

---

## 3. ComparaÃ§Ã£o Entre os Arquivos

| Arquivo            | Papel        | PersistÃªncia | Complexidade |
| ------------------ | ------------ | ------------ | ------------ |
| WebBaseLoader      | Coleta web   | NÃ£o          | Baixa        |
| PyPDFLoader        | Coleta local | NÃ£o          | Baixa        |
| Ingestion PGVector | IndexaÃ§Ã£o    | Sim          | Alta         |
| Search Vector      | RecuperaÃ§Ã£o  | Sim          | MÃ©dia        |

---

## 4. Aspectos Dependentes vs Independentes de Modelo

### ğŸ”’ Dependentes de modelo

* Classe `GoogleGenerativeAIEmbeddings`
* Nome do modelo (`text-embedding-004`)

### ğŸ”“ Independentes de modelo

* Loaders
* Splitters
* VectorStore API
* Interface `Document`

> O LangChain abstrai o modelo, permitindo troca futura por OpenAI, Cohere, HuggingFace, etc.

---

## 5. Boas PrÃ¡ticas e Dicas

### âœ… Boas prÃ¡ticas

* Sempre usar `.env`
* Validar variÃ¡veis de ambiente
* Usar splitters adequados ao caso
* Persistir metadados Ãºteis

### âŒ Erros comuns

* Chunks grandes demais
* Ignorar overlap
* Misturar ingestÃ£o e busca no mesmo script

---

## 6. Resumo Final

### ğŸ“Œ Principais aprendizados

* Loaders transformam dados brutos em documentos
* Splitters preparam dados para LLMs
* Embeddings capturam significado
* Bancos vetoriais permitem busca semÃ¢ntica

---

### âœ… Checklist mental do aluno

* [x] Sei carregar dados de web e PDF
* [x] Sei dividir textos corretamente
* [x] Entendo embeddings
* [x] Sei ingerir dados em banco vetorial
* [x] Sei consultar por similaridade semÃ¢ntica

---

ğŸ“˜ **ConclusÃ£o:** esta parte estabelece a base tÃ©cnica necessÃ¡ria para qualquer aplicaÃ§Ã£o sÃ©ria de RAG com LangChain.
