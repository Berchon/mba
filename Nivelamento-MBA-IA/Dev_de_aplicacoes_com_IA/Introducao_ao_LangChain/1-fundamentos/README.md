# ğŸ“˜ Parte 1 â€“ Fundamentos do LangChain

## ğŸ¯ Objetivo da Parte

Esta primeira parte do estudo tem como objetivo apresentar os **conceitos fundamentais do LangChain**, que servem como base sÃ³lida para qualquer aplicaÃ§Ã£o mais avanÃ§ada envolvendo **LLMs (Large Language Models)** e **Chat Models**.

Ao final desta parte, vocÃª serÃ¡ capaz de:

* ğŸ”Œ Inicializar e utilizar modelos de linguagem via LangChain
* ğŸ¤– Entender claramente a diferenÃ§a entre **LLMs tradicionais** e **Chat Models**
* ğŸ§± Compreender como o LangChain abstrai provedores e modelos
* ğŸ§© Criar prompts reutilizÃ¡veis usando **PromptTemplate** e **ChatPromptTemplate**
* ğŸ§  Preparar seu cÃ³digo para evoluir para **chains, memory e agents**

Esses fundamentos sÃ£o **obrigatÃ³rios** antes de avanÃ§ar para aplicaÃ§Ãµes reais.

---

## ğŸŒ O Papel dos Fundamentos no Ecossistema do LangChain

O LangChain atua como uma **camada de abstraÃ§Ã£o e orquestraÃ§Ã£o** sobre modelos de linguagem.

Antes de construir fluxos complexos, agentes autÃ´nomos ou aplicaÃ§Ãµes com memÃ³ria, Ã© essencial dominar:

* Como conversar com um modelo
* Como estruturar prompts corretamente
* Como separar **lÃ³gica de negÃ³cio** de **detalhes do modelo**
* Como escrever cÃ³digo desacoplado, reutilizÃ¡vel e sustentÃ¡vel

ğŸ‘‰ Os arquivos desta parte mostram uma **progressÃ£o pedagÃ³gica intencional**, indo do uso mais direto atÃ© abstraÃ§Ãµes mais profissionais.

---

## ğŸ“‚ VisÃ£o Geral dos Arquivos

| Arquivo              | Conceito Principal                | Papel DidÃ¡tico                 |
| -------------------- | --------------------------------- | ------------------------------ |
| `hello-world.py`     | Uso direto de um Chat Model       | Primeiro contato com LangChain |
| `init-chat-model.py` | InicializaÃ§Ã£o genÃ©rica de modelos | Desacoplamento e portabilidade |
| `prompt-template.py` | PromptTemplate                    | Prompts reutilizÃ¡veis          |
| `prompt-template.py` | ChatPromptTemplate                | Prompts estruturados para chat |

---

## 1ï¸âƒ£ hello-world.py

### ğŸ“Œ O que este cÃ³digo faz?

Realiza a **primeira interaÃ§Ã£o com um Chat Model**, usando diretamente uma classe especÃ­fica do provedor Google (Gemini).

Ã‰ o clÃ¡ssico **Hello World do LangChain**.

---

### ğŸ“ CÃ³digo

```python
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI

load_dotenv()

model = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.5)
message = model.invoke("Hello, world!")

print(message)
print("=" * 30)
print(message.content)
```

---

### ğŸ§  Conceitos Demonstrados

* ğŸ” Carregamento de variÃ¡veis de ambiente (`load_dotenv`)
* ğŸ¤– Uso direto de um Chat Model
* ğŸ” Chamada sÃ­ncrona via `.invoke()`
* ğŸ“¦ DiferenÃ§a entre objeto de resposta e texto final

---

### ğŸ” ExplicaÃ§Ã£o Detalhada por Blocos

#### 1. ConfiguraÃ§Ã£o do ambiente

```python
load_dotenv()
```

Carrega chaves de API e configuraÃ§Ãµes sensÃ­veis a partir do arquivo `.env`.

---

#### 2. InicializaÃ§Ã£o do modelo

```python
model = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.5
)
```

* Classe especÃ­fica do provedor Google
* `temperature` controla o grau de criatividade

---

#### 3. Chamada ao modelo

```python
message = model.invoke("Hello, world!")
```

* `.invoke()` Ã© a interface padrÃ£o do LangChain
* Retorna um **AIMessage**, nÃ£o apenas uma string

---

#### 4. Acesso ao conteÃºdo

```python
message.content
```

* Texto puro da resposta
* Essencial para processamento posterior

---

### âœ… Quando usar essa abordagem?

* Estudos iniciais
* Testes rÃ¡pidos
* Quando o provedor Ã© conhecido e fixo

### âŒ LimitaÃ§Ã£o

CÃ³digo fortemente **acoplado** ao provedor Google.

---

## 2ï¸âƒ£ init-chat-model.py

### ğŸ“Œ O que este cÃ³digo faz?

Inicializa um Chat Model usando uma **fÃ¡brica genÃ©rica do LangChain**, desacoplando o cÃ³digo do provedor especÃ­fico.

---

### ğŸ“ CÃ³digo

```python
from dotenv import load_dotenv
from langchain.chat_models import init_chat_model

load_dotenv()

chat_model = init_chat_model(
    model="gemini-2.5-flash",
    model_provider="google_genai"
)

answer = chat_model.invoke("Hello, world!")

print(answer)
print("=" * 30)
print(answer.content)
```

---

### ğŸ§  Conceitos Demonstrados

* ğŸ§± AbstraÃ§Ã£o de provedores
* ğŸ”„ InicializaÃ§Ã£o genÃ©rica de chat models
* â™»ï¸ CÃ³digo portÃ¡til e reutilizÃ¡vel

---

### ğŸ” O que muda em relaÃ§Ã£o ao cÃ³digo anterior?

| Aspecto          | hello-world.py | init-chat-model.py |
| ---------------- | -------------- | ------------------ |
| Classe           | EspecÃ­fica     | GenÃ©rica           |
| Acoplamento      | Alto           | Baixo              |
| Portabilidade    | Baixa          | Alta               |
| Uso profissional | Limitado       | Recomendado        |

---

### âœ… Quando usar essa abordagem?

* Projetos reais
* CÃ³digo de produÃ§Ã£o
* Ambientes com troca de modelo/provedor

ğŸ’¡ **Boa prÃ¡tica**: prefira `init_chat_model` sempre que possÃ­vel.

---

## 3ï¸âƒ£ prompt-template.py â€” PromptTemplate

### ğŸ“Œ O que este cÃ³digo faz?

Cria **prompts dinÃ¢micos e reutilizÃ¡veis**, separando texto de lÃ³gica.

---

### ğŸ“ CÃ³digo

```python
from langchain.prompts import PromptTemplate

template = PromptTemplate(
    input_variables=["name"],
    template="Hi, I'm {name}! Tell me a joke about my name!"
)

text = template.format(name="Aldebaran")
print(text)
```

---

### ğŸ§  Conceitos Demonstrados

* ğŸ§© SeparaÃ§Ã£o entre prompt e lÃ³gica
* ğŸ” ParametrizaÃ§Ã£o de prompts
* â™»ï¸ ReutilizaÃ§Ã£o em mÃºltiplos contextos

---

### ğŸ” ExplicaÃ§Ã£o Detalhada

* `input_variables`: define os parÃ¢metros obrigatÃ³rios
* `{name}`: placeholder dinÃ¢mico
* `format()`: gera o prompt final

---

### âœ… Quando usar PromptTemplate?

* Sempre que houver variÃ¡veis
* Em chains e agents
* Em qualquer aplicaÃ§Ã£o real

âŒ Evite prompts hardcoded em produÃ§Ã£o.

---

## 4ï¸âƒ£ ChatPromptTemplate â€” Prompt para Chat Models

### ğŸ“Œ O que este cÃ³digo faz?

Cria prompts **estruturados por papÃ©is**, prÃ³prios para Chat Models.

---

### ğŸ“ CÃ³digo

```python
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import ChatPromptTemplate

load_dotenv()

system = ("system", "you are an assistant that answers questions in a {style} style")
user = ("user", "{question}")

chat_prompt = ChatPromptTemplate([system, user])

messages = chat_prompt.format_messages(
    style="funny",
    question="Who is Alan Turing?"
)

model = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.5)
result = model.invoke(messages)

print(result.content)
```

---

### ğŸ§  Conceitos Demonstrados

* ğŸ­ PapÃ©is explÃ­citos (`system`, `user`)
* ğŸ“¬ Lista de mensagens como entrada
* ğŸ”„ `format_messages()` vs `format()`

---

## ğŸ§© PromptTemplate vs ChatPromptTemplate

### Qual a diferenÃ§a e quando usar cada um?

Entender a diferenÃ§a entre **PromptTemplate** e **ChatPromptTemplate** Ã© essencial para escrever prompts corretos, reutilizÃ¡veis e alinhados com o tipo de modelo que vocÃª estÃ¡ utilizando.

Embora ambos sirvam para **construir prompts dinÃ¢micos**, eles atendem a **estruturas de entrada diferentes** e representam **paradigmas distintos de interaÃ§Ã£o** com modelos de linguagem.

---

### ğŸ§  VisÃ£o Geral

| Aspecto                        | PromptTemplate | ChatPromptTemplate |
| ------------------------------ | -------------- | ------------------ |
| Tipo de saÃ­da                  | `str` (texto)  | Lista de mensagens |
| Paradigma                      | Texto linear   | Conversacional     |
| PapÃ©is (system/user/assistant) | âŒ NÃ£o          | âœ… Sim              |
| CompatÃ­vel com Chat Models     | âš ï¸ Parcial     | âœ… Total            |
| Uso em chains modernas         | âš ï¸ Limitado    | âœ… Recomendado      |
| Uso em agents                  | âŒ Raro         | âœ… PadrÃ£o           |
| NÃ­vel de controle              | MÃ©dio          | Alto               |

---

### ğŸ“ PromptTemplate

**O que Ã©?**
Gera uma Ãºnica string de texto a partir de um template parametrizÃ¡vel.

```python
from langchain.prompts import PromptTemplate

template = PromptTemplate(
    input_variables=["topic"],
    template="Explique o conceito de {topic} para um iniciante."
)

prompt = template.format(topic="LangChain")
```

**Quando usar:**

* Texto simples
* Sem contexto conversacional
* Estudos iniciais

**LimitaÃ§Ãµes:**

* NÃ£o possui papÃ©is
* Menor controle de comportamento

---

### ğŸ’¬ ChatPromptTemplate

**O que Ã©?**
Cria uma lista estruturada de mensagens, adequada a Chat Models.

```python
from langchain.prompts import ChatPromptTemplate

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "VocÃª Ã© um professor de programaÃ§Ã£o."),
    ("human", "Explique o conceito de {topic} para um iniciante.")
])

messages = chat_prompt.format_messages(topic="LangChain")
```

**Quando usar:**

* Chat Models (Gemini, GPT, Claude)
* System prompt
* Chains, memory e agents

**Vantagens:**

* Controle fino
* EscalÃ¡vel
* PadrÃ£o moderno

---

### ğŸ¯ Regra de Ouro

> **Texto simples â†’ PromptTemplate**
> **Conversas e comportamento â†’ ChatPromptTemplate**

---

## ğŸ”„ ComparaÃ§Ã£o Geral das Abordagens

| Tema           | Simples           | AbstraÃ­do        |
| -------------- | ----------------- | ---------------- |
| Modelo         | Classe especÃ­fica | FÃ¡brica genÃ©rica |
| Prompt         | String literal    | Templates        |
| Escalabilidade | Baixa             | Alta             |

---

## ğŸ¤– DependÃªncia de Modelo

### ğŸ”’ Dependente de Chat Models

* Estrutura de mensagens
* PapÃ©is (system, user)
* `.content`

### ğŸ”“ Independente (LangChain)

* `.invoke()`
* PromptTemplate
* Chains
* Memory
* Agents

ğŸ’¡ O LangChain protege seu cÃ³digo contra mudanÃ§as de modelo.

---

## âœ… Boas PrÃ¡ticas Introduzidas

* ğŸ” Uso de `.env`
* ğŸ§± InicializaÃ§Ã£o genÃ©rica de modelos
* ğŸ§© SeparaÃ§Ã£o de prompts e lÃ³gica
* â™»ï¸ ReutilizaÃ§Ã£o desde o inÃ­cio

---

## ğŸ§  Resumo Final â€” Parte 1

### ğŸ“š Conceitos Aprendidos

* O papel do LangChain
* InicializaÃ§Ã£o de Chat Models
* Uso de `.invoke()`
* PromptTemplate vs ChatPromptTemplate
* AbstraÃ§Ã£o de provedores

---

### âœ”ï¸ Checklist Completo de Conhecimento

* [x] Sei explicar a diferenÃ§a entre LLM e Chat Model
* [x] Sei inicializar modelos direta e genericamente
* [x] Entendo o papel do `load_dotenv`
* [x] Sei o que `.invoke()` retorna
* [x] Sei acessar `message.content`
* [x] Sei criar PromptTemplate
* [x] Sei criar ChatPromptTemplate
* [x] Sei quando usar string vs mensagens
* [x] Entendo o que Ã© dependente de modelo
* [x] Estou pronto para avanÃ§ar para Chains

---

ğŸš€ Com esses fundamentos dominados, vocÃª estÃ¡ pronto para avanÃ§ar no LangChain.
